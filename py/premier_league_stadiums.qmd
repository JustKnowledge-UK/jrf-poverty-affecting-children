---
title: "Premier League Stadia and Poverty"
author: Jolyon Miles-Wilson
date: 2025-07-01
format: html
---

This script scrapes a table of Premier League Stadium locations from Wikipedia and joins it with Children in Low Income Families data to explore poverty in the vicinity of stadia across the country.

```{python}
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re
import geopandas as gpd
from shapely.geometry import Point
import matplotlib.pyplot as plt
import matplotlib as mpl
import contextily as cx
from adjustText import adjust_text
import configparser
import psycopg2
import os
import pickle
import numpy as np
from sklearn.neighbors import BallTree, radius_neighbors_graph
from scipy.spatial import cKDTree
import janitor


config = configparser.ConfigParser()
config.read(os.path.join('..', 'db_config_jk.ini'))

db_params = dict(config['postgresql'])

with open(os.path.join('..', 'jk_primary_colours.txt'), 'r') as file:
    jk_colours = [line.strip() for line in file]

mpl.rcParams['font.family'] = 'Open Sans'

mpl.rcParams['font.size'] = 16
```

Read the table on wikipedia: [https://en.wikipedia.org/wiki/List_of_Premier_League_stadiums](https://en.wikipedia.org/wiki/List_of_Premier_League_stadiums)

```{python}
file = os.path.join('..','data','stadia_gdf.pkl')
if os.path.isfile(file)==False:
    # Define the URL
    url = "https://en.wikipedia.org/wiki/List_of_Premier_League_stadiums"

    # Send GET request to the webpage
    response = requests.get(url)
    response.raise_for_status()  # Raise an exception for bad status codes

    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')

    # Find the first wikitable (which contains the stadiums data)
    table = soup.find('table', {'class': 'wikitable'})

    # Extract table data using pandas
    stadiums_df = pd.read_html(str(table))[0]

    # Clean the data
    def clean_text(text):
        """Remove Wikipedia reference numbers and extra whitespace"""
        if pd.isna(text):
            return text
        # Remove reference numbers like [1], [2], etc.
        text = re.sub(r'\[.*?\]', '', str(text))
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    # Apply cleaning to all columns
    for col in stadiums_df.columns:
        stadiums_df[col] = stadiums_df[col].apply(clean_text)

    # Remove any completely empty rows
    stadiums_df = stadiums_df.dropna(how='all')

    # Display basic info about the scraped data
    print(f"Successfully scraped {len(stadiums_df)} stadiums")
    print(f"Columns: {', '.join(stadiums_df.columns)}")
    print("\nFirst few rows:")
    print(stadiums_df.head())

    # Display data types and info
    print("\nDataFrame info:")
    print(stadiums_df.info())

    # Optional: Save to CSV
    stadiums_df.to_csv('premier_league_stadiums.csv', index=False)
    print("\nData saved to 'premier_league_stadiums.csv'")

    # Parse the coordinates into usable WGS64 geometries
    def parse_coordinates(coord_str):
        """
        Convert coordinates to decimal degrees (WGS84)
        Handles format: 53°25′51″N 002°57′39″W﻿ / ﻿53.43083°N 2.96083°W
        """
        if pd.isna(coord_str) or coord_str == '':
            return None, None
        
        try:
            # Debug: Show what we're trying to parse
            print(f"DEBUG: Parsing coordinate string: '{coord_str}'")
            print(f"DEBUG: Length: {len(str(coord_str))}")
            print(f"DEBUG: Repr: {repr(str(coord_str))}")
            
            # Clean the string - remove extra whitespace and invisible characters
            coord_str = re.sub(r'\s+', ' ', str(coord_str).strip())
            coord_str = coord_str.replace('\ufeff', '')  # Remove BOM character if present
            coord_str = coord_str.replace('\u200f', '')  # Remove right-to-left mark
            coord_str = coord_str.replace('\u200e', '')  # Remove left-to-right mark
            
            print(f"DEBUG: After cleaning: '{coord_str}'")
            
            # Check if there's a "/" separator (indicating both DMS and decimal formats)
            if '/' in coord_str:
                print("DEBUG: Found '/' separator, trying decimal part")
                # Split and use the decimal part (after the "/")
                decimal_part = coord_str.split('/')[-1].strip()
                print(f"DEBUG: Decimal part: '{decimal_part}'")
                
                # More flexible pattern for decimal coordinates
                # Matches: 53.43083°N 2.96083°W or 53.43083°N, 2.96083°W
                decimal_patterns = [
                    r'(\d+\.?\d*)°([NS])\s+(\d+\.?\d*)°([EW])',
                    r'(\d+\.?\d*)°([NS])[,\s]+(\d+\.?\d*)°([EW])',
                    r'(\d+\.?\d*)\s*°\s*([NS])\s+(\d+\.?\d*)\s*°\s*([EW])'
                ]
                
                for pattern in decimal_patterns:
                    decimal_match = re.search(pattern, decimal_part)
                    if decimal_match:
                        print(f"DEBUG: Decimal pattern matched: {decimal_match.groups()}")
                        lat_val, lat_dir, lon_val, lon_dir = decimal_match.groups()
                        lat_decimal = float(lat_val)
                        lon_decimal = float(lon_val)
                        
                        # Apply direction (negative for South/West)
                        if lat_dir == 'S':
                            lat_decimal = -lat_decimal
                        if lon_dir == 'W':
                            lon_decimal = -lon_decimal
                            
                        print(f"DEBUG: Converted to: {lat_decimal}, {lon_decimal}")
                        return lat_decimal, lon_decimal
            
            # Fallback: Try to parse DMS format from the beginning
            print("DEBUG: Trying DMS format")
            # More flexible DMS patterns
            dms_patterns = [
                r'(\d+)°(\d+)′(\d+)″([NS])\s+(\d+)°(\d+)′(\d+)″([EW])',
                r'(\d+)°(\d+)′(\d+)″([NS])[,\s]+(\d+)°(\d+)′(\d+)″([EW])',
                r'(\d+)\s*°\s*(\d+)\s*′\s*(\d+)\s*″\s*([NS])\s+(\d+)\s*°\s*(\d+)\s*′\s*(\d+)\s*″\s*([EW])'
            ]
            
            for pattern in dms_patterns:
                dms_match = re.search(pattern, coord_str)
                if dms_match:
                    print(f"DEBUG: DMS pattern matched: {dms_match.groups()}")
                    lat_deg, lat_min, lat_sec, lat_dir, lon_deg, lon_min, lon_sec, lon_dir = dms_match.groups()
                    
                    # Convert to decimal degrees
                    lat_decimal = int(lat_deg) + int(lat_min)/60 + int(lat_sec)/3600
                    lon_decimal = int(lon_deg) + int(lon_min)/60 + int(lon_sec)/3600
                    
                    # Apply direction (negative for South/West)
                    if lat_dir == 'S':
                        lat_decimal = -lat_decimal
                    if lon_dir == 'W':
                        lon_decimal = -lon_decimal
                        
                    print(f"DEBUG: Converted to: {lat_decimal}, {lon_decimal}")
                    return lat_decimal, lon_decimal
            
            # Last attempt: simple decimal format
            print("DEBUG: Trying simple decimal format")
            simple_patterns = [
                r'([-+]?\d*\.?\d+)[,\s]+([-+]?\d*\.?\d+)',
                r'(\d+\.?\d+)\s*,\s*(\d+\.?\d+)',
                r'(\d+\.?\d+)\s+(\d+\.?\d+)'
            ]
            
            for pattern in simple_patterns:
                simple_match = re.search(pattern, coord_str)
                if simple_match:
                    print(f"DEBUG: Simple decimal pattern matched: {simple_match.groups()}")
                    lat, lon = simple_match.groups()
                    result = float(lat), float(lon)
                    print(f"DEBUG: Converted to: {result}")
                    return result
            
            print("DEBUG: No patterns matched")
                
        except Exception as e:
            print(f"ERROR parsing coordinates '{coord_str}': {e}")
            
        return None, None

    # Debug: Print all column names to see what's available
    print("All columns in the dataframe:")
    for i, col in enumerate(stadiums_df.columns):
        print(f"{i}: '{col}'")

    # Find coordinates column and convert to WGS84
    coord_columns = [col for col in stadiums_df.columns if col == 'Coordinates'] 

    coord_col = coord_columns[0]  # Use first coordinates column found
    print(f"Using coordinates column: '{coord_col}'")

    # Debug: Show some raw coordinate values
    print(f"\nSample raw coordinate values from '{coord_col}':")
    sample_coords = stadiums_df[coord_col].dropna().head(5)
    for i, coord in enumerate(sample_coords):
        print(f"{i+1}: '{coord}'")

    # Extract and convert coordinates
    coords = stadiums_df[coord_col].apply(parse_coordinates)

    # Debug: Show parsing results
    print(f"\nParsing results (first 5):")
    for i, coord in enumerate(coords.head(5)):
        original = stadiums_df[coord_col].iloc[i] if i < len(stadiums_df) else "N/A"
        print(f"{i+1}: '{original}' -> {coord}")

    # Create separate latitude and longitude columns
    stadiums_df['Latitude_WGS84'] = [coord[0] if coord[0] is not None else None for coord in coords]
    stadiums_df['Longitude_WGS84'] = [coord[1] if coord[1] is not None else None for coord in coords]

    # Display conversion results
    successful_conversions = stadiums_df[['Latitude_WGS84', 'Longitude_WGS84']].dropna()
    print(f"\nSuccessfully converted {len(successful_conversions)} coordinates to WGS84 decimal degrees")

    if len(successful_conversions) > 0:
        print("\nSample WGS84 coordinates:")
        print(successful_conversions.head())
    else:
        print("No coordinates were successfully converted. Check the parsing function.")

    # Optional: Display some basic statistics
    print(f"\nSample of stadium names:")
    if 'Stadium' in stadiums_df.columns:
        print(stadiums_df['Stadium'].head(5).tolist())
    elif len(stadiums_df.columns) > 0:
        print(stadiums_df.iloc[:5, 0].tolist())  # First column if 'Stadium' not found

    # Create GeoPandas GeoDataFrame
    successful_coords = stadiums_df.dropna(subset=['Latitude_WGS84', 'Longitude_WGS84'])

    if len(successful_coords) > 0:
        print(f"\nCreating GeoPandas GeoDataFrame with {len(successful_coords)} stadiums...")
        
        # Create Point geometries from lat/lon coordinates
        geometry = [Point(lon, lat) for lon, lat in zip(successful_coords['Longitude_WGS84'], 
                                                    successful_coords['Latitude_WGS84'])]
        
        # Create GeoDataFrame
        gdf = gpd.GeoDataFrame(successful_coords, geometry=geometry, crs='EPSG:4326')
        
        print(f"GeoPandas GeoDataFrame created successfully!")
        print(f"CRS: {gdf.crs}")
        print(f"Geometry type: {gdf.geometry.geom_type.iloc[0] if len(gdf) > 0 else 'None'}")
        
        # Display basic info about the GeoDataFrame
        print(f"\nGeoDataFrame info:")
        print(f"Shape: {gdf.shape}")
        print(f"Columns: {list(gdf.columns)}")
        
        # Show first few rows with geometry
        print(f"\nFirst few stadiums with coordinates:")
        if 'Stadium' in gdf.columns:
            display_cols = ['Stadium', 'Latitude_WGS84', 'Longitude_WGS84', 'geometry']
            available_cols = [col for col in display_cols if col in gdf.columns]
            print(gdf[available_cols].head())
        else:
            print(gdf[['Latitude_WGS84', 'Longitude_WGS84', 'geometry']].head())
        
        # Optional: Save as GeoJSON
        gdf.to_file('premier_league_stadiums.geojson', driver='GeoJSON')
        print(f"\nGeoDataFrame saved as 'premier_league_stadiums.geojson'")
        
        # Optional: Save as Shapefile
        try:
            gdf.to_file('premier_league_stadiums.shp')
            print(f"GeoDataFrame saved as 'premier_league_stadiums.shp'")
        except Exception as e:
            print(f"Could not save as Shapefile: {e}")
        
        # Calculate bounding box
        bounds = gdf.total_bounds
        print(f"\nBounding box (min_lon, min_lat, max_lon, max_lat): {bounds}")
        
        # Optional: Basic spatial analysis
        print(f"\nBasic spatial info:")
        print(f"Centroid of all stadiums: {gdf.geometry.centroid.iloc[0] if len(gdf) > 0 else 'None'}")
        
        # Return the GeoDataFrame for further use
        stadiums_gdf = gdf
        
    else:
        print("No valid coordinates found - cannot create GeoDataFrame")
        stadiums_gdf = None

    # Filter to just stadiums that are currently open
    stadiums_gdf = stadiums_gdf.loc[stadiums_gdf['Closed'].isna()]

    # Save the gdf
    stadiums_gdf.to_pickle(os.path.join('..','data','stadia_gdf.pkl'))
else:
    with open(file, 'rb') as picklefile:
        stadiums_gdf = pickle.load(picklefile)

```

# Plot

The plot below shows the Premier League Stadia listed on Wikipedia

```{python}
# Create figure and axis
fig, ax = plt.subplots(figsize=(10, 8))  # Note: subplots, not plt, and figsize should be reasonable

# Plot the stadiums
stadiums_gdf.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)

# Add some styling
ax.set_title('Premier League Stadiums', fontsize=16, fontweight='bold')
ax.set_axis_off()
cx.add_basemap(ax, crs = stadiums_gdf.crs, source=cx.providers.OpenStreetMap.Mapnik)

# Create text objects for labels
# texts = []
# for idx, row in stadiums_gdf.iterrows():
#     text = ax.text(row.geometry.x, row.geometry.y, row['Stadium'],
#                    fontsize=8, ha='center', va='center',
#                    bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))
#     texts.append(text)

# # Adjust text positions to avoid overlaps
# adjust_text(texts, 
#             arrowprops=dict(arrowstyle='->', color='black', lw=0.5),
#             expand_points=(1.2, 1.2),
#             expand_text=(1.2, 1.2))

# Show the plot
plt.tight_layout()
plt.show()
```

```{python}
tott = stadiums_gdf.loc[stadiums_gdf['Stadium'] == 'Tottenham Hotspur Stadium']

fig, ax = plt.subplots(figsize=(10, 8)) 

# Plot the stadiums
tott.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)


# Add some styling
ax.set_title('Premier League Stadiums', fontsize=16, fontweight='bold')


# Get the coordinates
x, y = tott.geometry.x.iloc[0], tott.geometry.y.iloc[0]
# Set reasonable axis limits around the point
margin = 0.01  # Adjust this value to zoom in/out
ax.set_xlim(x - margin, x + margin)
ax.set_ylim(y - margin, y + margin)

ax.set_axis_off()

cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)
plt.show()
```


```{python}

file = 'msoa_geometries_restored.pkl'
path = os.path.join('..', 'data', file)
if(os.path.isfile(path)) == False:
    print('Downloading geometries')
    # Get msoa geometries for the whole country, join with lad21 and rgn21
    query = '''SELECT foo.msoa21cd, foo.msoa21nm, poo.lad21nm, poo.rgn21nm, foo.geometry 
            FROM msoa21_boundaries foo
            LEFT JOIN (
            SELECT DISTINCT msoa21cd, lad21cd 
            FROM pcode_census21_lookup) loo
            ON foo.msoa21cd = loo.msoa21cd
            LEFT JOIN 
            (SELECT DISTINCT lad21nm, lad21cd, rgn21nm FROM lad21_lookup) poo
            ON loo.lad21cd = poo.lad21cd
    '''

    with psycopg2.connect(**db_params) as con:
        msoa_gdf = gpd.read_postgis(query, con = con, geom_col='geometry')

    msoa_gdf.to_pickle(path)
else:
    print('Geometries already in directory. Loading.')
    with open(path, 'rb') as file:
        msoa_gdf = pickle.load(file)
```

Use spatial join to locate stadiums within MSOAs

```{python}

# subset to only the current premier league satdiums
clubs = [
    'Emirates Stadium',
    'Villa Park',
    'Dean Court',
    'Brentford Community Stadium',
    'Falmer Stadium',
    'Stamford Bridge',
    'Selhurst Park',
    'Goodison Park',
    'Craven Cottage',
    'Portman Road',
    'King Power Stadium Formerly Walkers Stadium',
    'Anfield',
    'City of Manchester Stadium',
    'Old Trafford',
    "St James' Park",
    'City Ground',
    "St Mary's Stadium",
    'Tottenham Hotspur Stadium',
    'London Stadium Formerly Olympic Stadium',
    'Molineux Stadium'
]

points_gdf = gpd.GeoDataFrame(stadiums_gdf.loc[stadiums_gdf['Stadium'].isin(clubs), ['Stadium', 'geometry']]).reset_index(drop=True)

# points_gdf = gpd.GeoDataFrame(stadiums_gdf[['geometry']])
points_gdf = points_gdf.to_crs(epsg=27700) # maybe go back to web scraping and redefine the desired crs to 27700

# Spatial join to find which polygon each point falls in
joined = gpd.sjoin(points_gdf, msoa_gdf, how='left', predicate='within')

# with psycopg2.connect(**db_params) as con:
#     joined.to_postgis(name="premier_league_stadia_locations", con=con, if_exists="replace", index=False)

joined.to_pickle(os.path.join('..','data','stadium_locations.pkl'))

```

# Finding nearest neighbours

```{python}
# Get point coordinates (n_targets x 2)
target_coords = np.array([[geom.x, geom.y] for geom in joined.geometry])

# Radius in meters
radius_m = 1000

# Specify a buffer and find the MSOAs that intersect it
neighbors_dict = {}
for i, row in joined.iterrows():
    point_geom = row.geometry
    point_id = row.get('Stadium', f'point_{i}')
    
    # Create buffer around the point
    point_buffer = point_geom.buffer(radius_m)
    
    # Find all MSOAs that intersect with the buffer
    intersecting_msoas = msoa_gdf[msoa_gdf.geometry.intersects(point_buffer)]
    neighbor_codes = intersecting_msoas['msoa21cd'].tolist()
    
    neighbors_dict[point_id] = neighbor_codes

results = []
for i, row in joined.iterrows():
    point_id = row.get('Stadium', f'point_{i}')
    containing_msoa = row['msoa21cd']  # MSOA containing the point (may be NaN if outside)
    neighbors = neighbors_dict.get(point_id, [])
    results.append({
        'stadium': point_id,
        'containing_msoa': containing_msoa,
        'neighboring_msoas': neighbors
    })
```


```{python}
from collections import defaultdict
# Build a mapping from MSOA to associated stadium(s)
msoa_to_stadiums = defaultdict(set)
for res in results:
    stadium = res['stadium']
    if res['containing_msoa']:
        msoa_to_stadiums[res['containing_msoa']].add(stadium)
    for neighbor in res['neighboring_msoas']:
        # print(neighbor)
        msoa_to_stadiums[neighbor].add(stadium)

# Flatten the mapping into a DataFrame
msoa_stadium_rows = []
for msoa, stadiums in msoa_to_stadiums.items():
    for stadium in stadiums:
        msoa_stadium_rows.append({
            'msoa21cd': msoa,
            'stadium': stadium
        })

msoa_stadium_df = pd.DataFrame(msoa_stadium_rows)

# Join this info with the original MSOA GeoDataFrame
neighbors_gdf = msoa_gdf.merge(msoa_stadium_df, on='msoa21cd', how='inner')
neighbors_gdf.to_pickle(os.path.join('..','data','stadium_neighbours.pkl'))
```

Plot Haringey to check how the neighbour identification has gone

```{python}
tott = stadiums_gdf.loc[stadiums_gdf['Stadium'] == 'Tottenham Hotspur Stadium']
# Plot Haringey to check
haringey = neighbors_gdf.loc[neighbors_gdf['stadium'].str.contains('Tottenham')]
tott = tott.to_crs(epsg=27700)

fig, ax = plt.subplots(figsize=[8,8])

haringey.plot(ax=ax, alpha=0.5, edgecolor="black")
tott.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)
cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)

```

# Child poverty data

```{python}
relative = pd.read_csv(os.path.join('..','data','relative_msoa_u15-23-24_codes.csv'), skiprows=9, skipfooter=15)

relative = relative.iloc[:,[1,2,4]]
relative.columns = ['msoa_code','age','count']
relative = relative.loc[relative['age']=='Total']
relative = relative.drop(columns='age')
relative['count'] = relative['count'].replace('..', 0).astype(int)


absolute = pd.read_csv(os.path.join('..','data','absolute_low_income_msoa.csv'), skiprows=9)

absolute = absolute.iloc[:,[1,3]]
absolute.columns = ['msoa_name','count']
```



Get population estimates for under 16s in small areas. NOMIS API request would be preferable but there are issues with joining alter with this approach. In lieu I've used the census bulk downloads, Table 007 from https://www.nomisweb.co.uk/sources/census_2021_bulk

```{python}
# Get population estimates via NOMIS API
# file = os.path.join('..','data', 'london_population_msoa.pkl')
# if os.path.isfile(file)==False:   
#     # Build the query
#     base = "https://www.nomisweb.co.uk/api/v01/dataset/NM_2014_1.csv?"
#     date = "date=latest"
#     age = "c_age=101...191"
#     gender = "gender=0"
#     measures = "measures=20100"

#     pop_ests_list = []

#     # Chunk the query because API doesn't want long strings
#     chunk_size = 100
#     for i in tqdm.tqdm(range(0, len(london_msoas), chunk_size)):
#         chunk = london_msoas[i:i + chunk_size]
#         geography = 'geography=' + ','.join(chunk)
#         query = base + geography + "&" + date + "&" + gender + "&" + age + "&" + measures

#         req = requests.get(query)
#         chunk_df = pd.read_csv(io.StringIO(req.content.decode('utf-8')))
#         pop_ests_list.append(chunk_df)

#     pop_ests = pd.concat(pop_ests_list, ignore_index=True)

#     pop_ests.to_pickle(file)
# else:
#     print('Data already acquired. Loading it')
#     with open(file, 'rb') as picklefile:
#         pop_ests = pickle.load(picklefile)


# pop_ests = janitor.clean_names(pop_ests)

# # Drop unwanted columns
# cols_to_keep = ['geography_name','geography_code','c_age','c_age_name','obs_value']

# pop_ests = pop_ests[cols_to_keep]

# # Get just under 16s and sum the population so we have 1 per msoa
# pop_ests_u15 = pop_ests.loc[pop_ests['c_age'] <= 116]
# pop_ests_u15 = pop_ests_u15.groupby(['geography_name','geography_code'])['obs_value'].sum().reset_index()

# pop_ests_u15 = pop_ests_u15.rename(
#     columns = {
#         'geography_name':'msoa21nm',
#         'geography_code':'msoa21cd',
#         'obs_value':'population'
#     }
# )
```

```{python}
# NOMIS request is preferable but mis reuqest misses data for some reason
filepath = os.path.join('..', 'data', 'census2021-ts007-msoa.csv')
pop_ests = pd.read_csv(filepath)

pop_ests = janitor.clean_names(pop_ests)
pop_ests = pop_ests.rename(columns={
    'geography':'msoa21nm',
    'geography_code': 'msoa21cd'})

pop_ests.columns = (
    pop_ests.columns
    .str.replace(';_measures_value', '', regex=True)   # Remove ','

)

# Just get cols of interest
pop_ests = pop_ests[['msoa21nm','msoa21cd','age_aged_4_years_and_under','age_aged_5_to_9_years','age_aged_10_to_15_years']]

pop_ests['population'] = pop_ests[['age_aged_4_years_and_under','age_aged_5_to_9_years','age_aged_10_to_15_years']].sum(axis=1)

pop_ests = pop_ests[['msoa21nm','msoa21cd','population']]
```

```{python}
# Get MSOA11 to MSOA21 lookup
import time

def fetch_data(base_url, where_clause, result_offset, max_records):
    params = {
        'where': where_clause,
        'outFields': '*',
        'outSR': '4326',
        'f': 'json',
        'resultOffset': result_offset,
        'resultRecordCount': max_records
    }

    response = requests.get(base_url, params=params)
    error_504 = False

    if len(response.content) < 1000:
        peek_content = response.text
        if "error" in peek_content.lower() and "504" in peek_content:
            error_504 = True

    if response.status_code == 200 and not error_504:
        return response
    else:
        raise Exception(f"Error: Status code {response.status_code} (body may contain 504)")

# Main loop

api_endpoint = 'https://services1.arcgis.com/ESMARspQHYMw9BZ9/ArcGIS/rest/services/MSOA11_MSOA21_LAD22_EW_LU_v2/FeatureServer/0/query'

rep = 1
where_clause = '1=1'
result_offset = 0
max_records = 2000
max_tries = 5
all_results = []

while True:
    print(f"\nStarting page {rep}")
    attempt = 1
    current_max_records = max_records

    while attempt <= max_tries:
        print(f"\nAttempt {attempt}")
        print(f"Trying with max records = {current_max_records}")

        try:
            response = fetch_data(
                base_url=api_endpoint,
                where_clause=where_clause,
                result_offset=result_offset,
                max_records=current_max_records
            )
            print(f"\nSuccess with max records = {current_max_records}")
            break

        except Exception as e:
            print(f"Error: {e}")
            attempt += 1
            current_max_records = int(current_max_records / 2)
            print("Retrying...")
            time.sleep(1)

    if attempt > max_tries:
        print("Max attempts reached. Exiting...")
        break

    json_data = response.json()
    features = json_data.get('features', [])

    if not features:
        break

    # Extract attributes only
    records = [f['attributes'] for f in features]
    df_chunk = pd.DataFrame(records)

    if df_chunk.empty:
        break

    all_results.append(df_chunk)
    result_offset += len(df_chunk)
    rep += 1

# Concatenate all chunks into a single pandas DataFrame
if all_results:
    lookup = pd.concat(all_results, ignore_index=True)
    print(lookup.head())

lookup = janitor.clean_names(lookup)

```

```{python}
# crosswalk data to msoa21
# I'm not 100% sure this is the best way to do this
# It only takes account where msoa11s have merged to msoa21s.
# Doesn't take account where msoa11s have split into msoa21s?
# There are still 122 msoas in CILIF that don't carry over in the merge
# Must be due to splits?
relative2 = relative.merge(lookup, how='left', left_on='msoa_code', right_on='msoa11cd')

relative2 = relative2.groupby(['msoa21cd'])['count'].sum()
# result = relative2.groupby('msoa21cd', as_index=False).agg({'count': 'sum'})

```


```{python}
# Join pop ests to CILIF data
# Relative
relative2 = (relative
            .merge(pop_ests, how = 'left', left_on='msoa_code', right_on='msoa21cd')
            # .drop(columns='msoa_code')
)

# Move count to a better location
cols = list(relative2.columns)
cols.remove('count')
insert_at = cols.index('population')
cols.insert(insert_at, 'count')

relative2 = relative2[cols]

relative2['percentage'] = relative2['count'].div(relative2['population'])*100
relative2['rank'] = relative2['percentage'].rank(method='min', ascending=False)
relative2['decile'] = pd.qcut(relative2['rank'], q=10, labels=list(range(1,11,1)), duplicates='drop')
relative2['decile'] = pd.to_numeric(relative2['decile'], errors='coerce')

# Absolute
absolute2 = (absolute
            .merge(pop_ests, how = 'left', left_on='msoa_name', right_on='msoa21nm')
            .drop(columns='msoa_name')
)

# Move count to a better location
cols = list(absolute2.columns)
cols.remove('count')
insert_at = cols.index('population')
cols.insert(insert_at, 'count')

absolute2 = absolute2[cols]

absolute2['percentage'] = absolute2['count'].div(absolute2['population'])*100
absolute2['rank'] = absolute2['percentage'].rank(method='min')
absolute2['decile'] = pd.qcut(absolute2['percentage'], q=10, labels=list(range(10,0,-1)), duplicates='drop')
absolute2['decile'] = pd.to_numeric(absolute2['decile'], errors='coerce')

```



# Join neighbours with CILIF

```{python}
neighbors_gdf2 = (neighbors_gdf
                  .merge(relative2.loc[:,['msoa21nm','percentage','rank','decile']], how='left', on='msoa21nm')
                  .rename(columns={
                    'percentage':'relative_percentage',
                    'rank': 'relative_rank',
                    'decile': 'relative_decile'
                    })
                  .merge(absolute2.loc[:,['msoa21nm','percentage','rank','decile']], how='left', on='msoa21nm')
                  .rename(columns={
                    'percentage':'absolute_percentage',
                    'rank': 'absolute_rank',
                    'decile': 'absolute_decile'})
                  .sort_values(by='msoa21nm')
)
```


```{python}
# get lookup again
with psycopg2.connect(**db_params) as con:
    query = ''' SELECT DISTINCT msoa21cd, ladcd as lad21cd, ladnm 
            FROM pcode_census21_lookup
            '''

    msoa_lad_lookup = pd.read_sql(query, con=con)
```

```{python}
relative3 = relative2.merge(msoa_lad_lookup, how='left', on='msoa21cd')
```

# Plots

## All stadia

```{python}
# use relative2 because this is all MSOAs
decile_boundaries = relative2['rank'].quantile([i/10 for i in range(0, 11)]).tolist()

tott_dots = neighbors_gdf2.loc[neighbors_gdf2['stadium']=='Tottenham Hotspur Stadium']
np_dots = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd']=='E02000398']
non_tott_dots = neighbors_gdf2.loc[neighbors_gdf2['stadium'] !='Tottenham Hotspur Stadium']

las_stads = joined[['lad21nm','Stadium']].drop_duplicates()

la_avg_ranks = relative3.loc[relative3['ladnm'].isin(las_stads['lad21nm'].unique())]
la_avg_ranks = la_avg_ranks.groupby('ladnm')['rank'].mean().reset_index()
# Optional: rename for clarity
la_avg_ranks.rename(columns={'rank': 'average_rank'}, inplace=True)
la_avg_ranks = la_avg_ranks.merge(las_stads,how='left',left_on='ladnm', right_on='lad21nm')

fig, ax = plt.subplots(figsize=[9,9])

# Plot grey points first (bottom layer)

ax.scatter(tott_dots['relative_rank'], tott_dots['stadium'], c='black')
ax.scatter(non_tott_dots['relative_rank'], non_tott_dots['stadium'], c='lightgrey')
ax.scatter(np_dots['relative_rank'], np_dots['stadium'], c='red')
ax.scatter(la_avg_ranks['average_rank'], la_avg_ranks['Stadium'], marker="D",c='blue')

# Add vertical lines for decile boundaries
for i, boundary in enumerate(decile_boundaries):
    ax.axvline(
        x=boundary, 
        color='red', 
        # linestyle=(0, (5, 10)), 
        linestyle='dotted', 
        alpha=0.7)
    if i < 10:
        x_loc = (decile_boundaries[i] + decile_boundaries[i+1]) / 2
        ax.text(x=x_loc, y=ax.get_ylim()[0]*1.2, s=i+1, ha='center')

ax.text(x=ax.get_xlim()[0], y=ax.get_ylim()[0]*1.2, s='Decile', ha='right')
ax.text(0, ax.get_ylim()[1]*1.1, s='Red dot indicates Northumberland Park\nBlack dots indicate MSOAs within 1km of Tottenham Hotspur Stadium\nBlue diamonds indicate the average rank of the borough the Stadium is in', size=8)

# Get the mapping from stadium names to y-axis positions
nudge_x = 0
nudge_y = -0.3
y_labels = ax.get_yticklabels()
y_positions = ax.get_yticks()
label_mapping = {label.get_text(): pos for label, pos in zip(y_labels, y_positions)}

for i, row in la_avg_ranks.iterrows():
    if row['Stadium'] in label_mapping:
        y_pos = label_mapping[row['Stadium']]
        ax.text(x=row['average_rank'] + nudge_x, 
                y=y_pos + nudge_y, 
                s=row['ladnm'], ha='left')

# plt.figtext(0.38, 0.0015, 'Black dots indicate Northumberland Park LSOAs', wrap=False, horizontalalignment='right', fontsize=10)
ax.invert_yaxis()

plt.xlabel('Rank')
# Add title with specific positioning
plt.suptitle(
    'MSOAs within 1km of Premier League Stadiums', fontsize=14, 
    x = 0.7, y=.99, 
    ha='center')  # y controls vertical position
plt.tight_layout()
fig.savefig(os.path.join('..','outputs','all_stadiums_1km.png'), dpi=800, bbox_inches='tight')
plt.show()


```

## Just London stadia 

```{python}
colours = {
    'lad_avg' : {
        'name' : 'Teal',
        'colour' : jk_colours[0]
    },
    'decile_line' : {
        'name' : 'Black',
        'colour' : jk_colours[-1]
    },
    'highlight_colour' : {
        'name' : 'Pink',
        'colour' : jk_colours[3]
    }
}

# colours = {
#     'decile_line' : {
#         'name' : 'Teal',
#         'colour' : jk_colours[0]
#     },
#     'lad_avg' : {
#         'name' : 'Orange',
#         'colour' : jk_colours[2]
#     },
#     'highlight_colour' : {
#         'name' : 'Pink',
#         'colour' : jk_colours[3]
#     }
# }

# use relative2 because this is all MSOAs
decile_boundaries = relative2['rank'].quantile([i/10 for i in range(0, 11)]).tolist()

neighbors_gdf3 = neighbors_gdf2.loc[neighbors_gdf2['rgn21nm']=='London']

tott_dots = neighbors_gdf3.loc[neighbors_gdf3['stadium']=='Tottenham Hotspur Stadium']
np_dots = neighbors_gdf3.loc[neighbors_gdf3['msoa21cd']=='E02000398']
non_tott_dots = neighbors_gdf3.loc[neighbors_gdf3['stadium'] !='Tottenham Hotspur Stadium']


las_stads = joined.loc[joined['rgn21nm']=='London', ['lad21nm','Stadium']].drop_duplicates()

la_avg_ranks = relative3.loc[relative3['ladnm'].isin(las_stads['lad21nm'].unique())]
la_avg_ranks = la_avg_ranks.groupby('ladnm')['rank'].mean().reset_index()
# Optional: rename for clarity
la_avg_ranks.rename(columns={'rank': 'average_rank'}, inplace=True)
la_avg_ranks = la_avg_ranks.merge(las_stads,how='left',left_on='ladnm', right_on='lad21nm')

nudge_x = 0
nudge_y = -0.1
fig, ax = plt.subplots(figsize=[9,9])

# Plot grey points first (bottom layer)
dot_size = 5

ax.scatter(tott_dots['relative_rank'], tott_dots['stadium'], c='black', lw=dot_size)
ax.scatter(non_tott_dots['relative_rank'], non_tott_dots['stadium'], c='lightgrey', lw=dot_size)
ax.scatter(np_dots['relative_rank'], np_dots['stadium'], c=colours['highlight_colour']['colour'], lw=dot_size)
ax.scatter(la_avg_ranks['average_rank'], la_avg_ranks['Stadium'], marker="D",c=colours['lad_avg']['colour'], lw=dot_size)

# Add vertical lines for decile boundaries
for i, boundary in enumerate(decile_boundaries):
    ax.axvline(
        x=boundary, 
        color=colours['decile_line']['colour'], 
        # linestyle=(0, (5, 10)), 
        linestyle='dotted', 
        alpha=0.3)
    if i < 10:
        x_loc = (decile_boundaries[i] + decile_boundaries[i+1]) / 2
        ax.text(x=x_loc, y=ax.get_ylim()[0]*1.2, s=i+1, ha='center')

text_pos = 7264
ax.text(x=ax.get_xlim()[0], y=ax.get_ylim()[0]*1.2, s='Decile', ha='right')
ax.text(text_pos, ax.get_ylim()[1]*1.2, s=f'{colours['highlight_colour']['name']} dot indicates Northumberland Park\nBlack dots indicate MSOAs within 1km of Tottenham Hotspur Stadium\n{colours['lad_avg']['name']} diamonds indicate the average rank of the borough the Stadium is in', size=10, ha='right')
# for i, row in la_avg_ranks.iterrows():
#     ax.text(x=row['average_rank'] + nudge_x, 
#             y=row['Stadium'] + nudge_y, 
#             s=row['ladnm'], ha='left')

# Get the mapping from stadium names to y-axis positions
y_labels = ax.get_yticklabels()
y_positions = ax.get_yticks()
label_mapping = {label.get_text(): pos for label, pos in zip(y_labels, y_positions)}

for i, row in la_avg_ranks.iterrows():
    if row['Stadium'] in label_mapping:
        y_pos = label_mapping[row['Stadium']]
        ax.text(x=row['average_rank'] + nudge_x, 
                y=y_pos + nudge_y, 
                s=row['ladnm'], ha='left')


# plt.figtext(0.38, 0.0015, 'Black dots indicate Northumberland Park LSOAs', wrap=False, horizontalalignment='right', fontsize=10)
ax.invert_yaxis()

ax.get_xaxis().set_major_formatter(mpl.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))

plt.xlabel('Rank')
# Add title with specific positioning
# plt.suptitle(
#     'MSOAs within 1km of London Stadiums', fontsize=14, 
#     x = 0.7, y=.99, 
#     ha='center')  # y controls vertical position

# plt.tight_layout()
fig.savefig(os.path.join('..','outputs','london_stadiums_1km.png'), dpi=300, bbox_inches='tight')
plt.show()



```

## Tottenham and 1km surrounding 

```{python}
# Plot Tottenham agani with deciles this time
tott = stadiums_gdf.loc[stadiums_gdf['Stadium'] == 'Tottenham Hotspur Stadium']
haringey = neighbors_gdf2.loc[neighbors_gdf2['stadium'].str.contains('Tottenham')]
tott = tott.to_crs(epsg=27700)
north_park = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd']=='E02000398']
np_centroid = north_park['geometry'].centroid
x, y = np_centroid.x, np_centroid.y
np_rank = north_park['relative_rank'].values[0]
max_rank = int(relative2['rank'].max())
np_decile = north_park['relative_decile'].values[0]

fig, ax = plt.subplots(figsize=[8,8])

# Plot deciles
haringey.plot(
    column='relative_decile', 
    legend=True, legend_kwds={'label':'Decile'}, vmax = 10, vmin = 1,
    ax=ax, alpha=0.5, edgecolor="black")

# Plot stadium
tott.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)

# Titles etc.
plt.title('Child poverty in MSOAs within 1km of Tottenham Hotspur Stadium')
ax.text(x=x, y=y, s='Northumberland Park\nRank: '+ str(int(np_rank)) + '/' + str(max_rank) + '\nDecile: ' + str(int(np_decile)), ha='center', va='center', color='white', size = 9)
ax.text(x=ax.get_xlim()[0],y=ax.get_ylim()[0]-150, s='Red dot indicates Tottenham Hotspur Stadium')
cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)
ax.set_axis_off()

fig.savefig(os.path.join('..','outputs','tott_hs_1km_map.png'), dpi=300)

```

## All stadia and 1km buffer 

Reveals missing areas

Possible cause:

CILIF data is using 2011 MSOAs, population estimates are using 2021 MSOAs

Can we get pop ests in 2011 units?

Or we look up 2011 pop ests to 2021s


```{python}
for area, stadium in zip(las_stads.iloc[:,0], las_stads.iloc[:,1]):
    print(area + ':' + stadium)
    this_area_relative_gdf = neighbors_gdf2.loc[neighbors_gdf2['stadium']==stadium]
    this_area_stadiums_gdf = stadiums_gdf.loc[stadiums_gdf['Stadium'] == stadium]
    this_area_stadiums_gdf = this_area_stadiums_gdf.to_crs(epsg=27700)
    # north_park = neighbors_gdf2.loc[neighbors_gdf2['msoa21cd']=='E02000398']
    # np_centroid = north_park['geometry'].centroid
    # x, y = np_centroid.x, np_centroid.y
    # np_rank = north_park['relative_rank'].values[0]
    # max_rank = int(relative2['rank'].max())
    # np_decile = north_park['relative_decile'].values[0]

    fig, ax = plt.subplots(figsize=[8,8])

    # Plot deciles
    this_area_relative_gdf.plot(
        column='relative_decile', 
        legend=True, legend_kwds={'label':'Decile'}, vmax = 10, vmin = 1,
        ax=ax, alpha=0.5, edgecolor="black")

    # Plot stadium
    this_area_stadiums_gdf.plot(ax=ax, marker='o', color='red', markersize=50, alpha=0.7)

    # Titles etc.
    plt.title('Child poverty in MSOAs within 1km of ' + stadium)
    # ax.text(x=x, y=y, s='Northumberland Park\nRank: '+ str(int(np_rank)) + '/' + str(max_rank) + '\nDecile: ' + str(int(np_decile)), ha='center', va='center', color='white', size = 9)
    ax.text(x=ax.get_xlim()[0],y=ax.get_ylim()[0]-150, s='Red dot indicates ' + stadium +'\nStadium borough: ' + area, va='top')
    cx.add_basemap(ax, crs = tott.crs, source=cx.providers.OpenStreetMap.Mapnik)
    ax.set_axis_off()
```
